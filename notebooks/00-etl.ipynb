{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Transform, Load (ETL) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook provides the initial raw data processing for storage on GitHub. The focus is on extracting, combining, and loading the data and useful metadata.\n",
    "\n",
    "**Note:** The processed data is stored and available on GitHub as part of this repo, so this notebook is not necessary to run unless you want to update the data or add new datasets.\n",
    "\n",
    "Key steps are:\n",
    "-   Setting up paths (where to put the data)\n",
    "-   Downloading datasets from various sources\n",
    "-   Saving useful metadata\n",
    "-   Saving larger processed data as gzipped (geo)parquet files\n",
    "-   Saving smaller metadata / lookups / variable descriptions as plain CSV files\n",
    "\n",
    "\n",
    "## Datasets\n",
    "\n",
    "**MedSat**\n",
    "```mermaid\n",
    "graph LR;\n",
    "    classDef dataset fill:#ffccff,stroke:#333,stroke-width:2px;\n",
    "    classDef metadata fill:#c1e1c1,stroke:#333,stroke-width:2px;\n",
    "    classDef variables fill:#add8e6,stroke:#333,stroke-width:2px;\n",
    "\n",
    "    A[MedSat]:::dataset --> B[medsat.parquet.gzip]:::dataset\n",
    "    A --> C[medsat_variables.xlsx]:::metadata\n",
    "    A --> D[sociodemographic.csv]:::metadata\n",
    "    A --> E[environmental.csv]:::metadata\n",
    "    A --> F[outcomes.csv]:::metadata\n",
    "    A --> G[auxiliary.csv]:::metadata\n",
    "    A --> H[variables.csv]:::variables\n",
    "```\n",
    "\n",
    "**Spatial Signatures**\n",
    "```mermaid\n",
    "graph TD;\n",
    "    classDef dataset fill:#ffccff,stroke:#333,stroke-width:2px;\n",
    "    classDef metadata fill:#c1e1c1,stroke:#333,stroke-width:2px;\n",
    "    classDef variables fill:#add8e6,stroke:#333,stroke-width:2px;\n",
    "\n",
    "    A[Spatial Signatures]:::dataset --> B[Partitioned Parquet files]:::dataset\n",
    "    B --> B2[spatial_signatures_XX.parquet.gzip]:::dataset\n",
    "    A --> C[metadata.csv]:::metadata\n",
    "    A --> D[variables.csv]:::variables\n",
    "```\n",
    "\n",
    "**LSOA Lookups**\n",
    "```mermaid\n",
    "graph TD;\n",
    "    classDef dataset fill:#ffccff,stroke:#333,stroke-width:2px;\n",
    "    classDef metadata fill:#c1e1c1,stroke:#333,stroke-width:2px;\n",
    "\n",
    "    M[LSOA Lookups]:::dataset --> N[lsoa_to_bua_lad_region_lookup.csv]:::metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from ftplib import FTP\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from utils import find_best_match, clean_columns, cdrc_get_metadata, cdrc_login\n",
    "\n",
    "\n",
    "CSV_KWARGS = dict(index=False, encoding=\"utf-8\", quoting=2)\n",
    "\n",
    "# Set up paths\n",
    "TMP_PATH = Path(\"../data/_tmp\")\n",
    "MEDSAT_PATH = Path(\"../data/medsat\")\n",
    "SPATIAL_SIGNATURES_PATH = Path(\"../data/spatial_signatures\")\n",
    "\n",
    "for folder in (TMP_PATH, MEDSAT_PATH, SPATIAL_SIGNATURES_PATH):\n",
    "    folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedSat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://doi.org/10.14459/2023mp1714817\n",
    "host = \"dataserv.ub.tum.de\"\n",
    "user = \"m1714817\"\n",
    "password = user\n",
    "\n",
    "download_list = [\n",
    "    \"point_data/2019_spatial_raw_master.geojson\",\n",
    "    \"point_data/2020_spatial_raw_master.geojson\",\n",
    "    \"point_data/MedSat Variables.xlsx\",\n",
    "    \"auxiliary data/lsoas_regions_mapping.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FTP(host, user, password) as ftp:\n",
    "    for ftp_file in download_list:\n",
    "        out_path = TMP_PATH / ftp_file.split(\"/\")[-1]\n",
    "\n",
    "        if out_path.exists():\n",
    "            print(f\"File already exists: {out_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading {ftp_file} to {out_path}\")\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            ftp.retrbinary(f\"RETR {ftp_file}\", f.write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_to_region = pd.read_csv(\"../data/_tmp/lsoas_regions_mapping.csv\")[[\"LSOA21CD\", \"RGN22CD\", \"RGN22NM\"]]\n",
    "medsat_2019 = gpd.read_file(\"../data/_tmp/2019_spatial_raw_master.geojson\").assign(year=2019)\n",
    "medsat_2020 = gpd.read_file(\"../data/_tmp/2020_spatial_raw_master.geojson\").assign(year=2020)\n",
    "\n",
    "medsat_variables = pd.ExcelFile(MEDSAT_PATH / \"medsat_variables.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MedSat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    len(set(medsat_2019.columns) ^ set(medsat_2020.columns)) == 0\n",
    "), \"Columns do not match between Medsat Dataset Years\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsat = (\n",
    "    pd.concat([medsat_2019, medsat_2020], ignore_index=True)\n",
    "    .rename(columns={\"geography code\": \"LSOA21CD\"})\n",
    "    .merge(lsoa_to_region, on=\"LSOA21CD\", how=\"left\")\n",
    "    .sort_values(by=[\"RGN22CD\", \"LSOA21CD\", \"year\"], ignore_index=True)\n",
    "    # 54 ring self intersections:\n",
    "    .assign(geometry=lambda df: df['geometry'].make_valid())\n",
    ")\n",
    "\n",
    "# certain census (c_) columns have multiple spaces:\n",
    "medsat.columns = [re.sub(r'\\s+', ' ', c) for c in medsat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medsat Variables / Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets = medsat_variables.parse(\"INFO\", skiprows=[0, 1], usecols=[1, 2], names=[\"sheet_name\", \"sheet_description\"])\n",
    "sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sociodemographic Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sociodemographic = medsat_variables.parse(\"sociodemographic\", skiprows=[0, 1]).iloc[:, 1:]\n",
    "sociodemographic.columns = [c.lower().replace(\" \", \"_\") for c in sociodemographic.columns]\n",
    "\n",
    "sociodemographic = sociodemographic.rename(columns={\"source.1\": \"source_category\"}).assign(\n",
    "    more_info=lambda df: df[\"more_info\"].ffill(),\n",
    "    year=lambda df: df[\"year\"].ffill().astype(int),\n",
    "    type=lambda df: df[\"type\"].str.strip().str.replace('  ', ' ')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable names do not match medsat columns for around ~30 out of 111 sociodemographic columns\n",
    "# fuzzy matching used and manually checked as per:\n",
    "# sociodemographic[['name', 'mesdat_name']].to_clipboard()\n",
    "c_columns = set(medsat.filter(regex=\"^c_\").columns)\n",
    "sociodemographic[\"medsat_name\"] = sociodemographic[\"name\"].apply(find_best_match, args=(c_columns,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmental = medsat_variables.parse(\"environmental\", skiprows=[0]).iloc[:, 3:]\n",
    "environmental.columns = [c.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\") for c in environmental.columns]\n",
    "\n",
    "environmental[\"link_to_data_source\"] = environmental[\"link_to_data_source\"].ffill()\n",
    "environmental[\"medsat_name\"] = \"e_\" + environmental[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = medsat_variables.parse(\"outcomes\", skiprows=[0, 1]).iloc[:, 4:]\n",
    "outcomes.columns = [c.lower().replace(\" \", \"_\") for c in outcomes.columns]\n",
    "\n",
    "outcomes[\"medsat_name\"] = \"o_\" + outcomes[\"name\"].str.replace(\"quantitiy\", \"quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary = medsat_variables.parse(\"auxiliary\").iloc[:, 1:]\n",
    "auxiliary.columns = [c.lower().replace(\" \", \"_\") for c in auxiliary.columns]\n",
    "auxiliary[\"medsat_name\"] = auxiliary[\"name\"]\n",
    "\n",
    "auxiliary.loc[0] = {\n",
    "    \"name\": \"LSOA21CD\",\n",
    "    \"meaning\": \"LSOA code\",\n",
    "    \"medsat_name\": \"LSOA21CD\",\n",
    "}\n",
    "new_fields = pd.DataFrame(\n",
    "    [\n",
    "        {\"name\": \"RGN22CD\", \"meaning\": \"Region code\", \"medsat_name\": \"RGN22CD\"},\n",
    "        {\"name\": \"RGN22NM\", \"meaning\": \"Region name\", \"medsat_name\": \"RGN22NM\"},\n",
    "        {\"name\": \"year\", \"meaning\": \"year of medsat data\", \"medsat_name\": \"year\"},\n",
    "    ]\n",
    ")\n",
    "auxiliary = pd.concat([auxiliary, new_fields], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Variable Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    sociodemographic[[\"medsat_name\", \"type\"]].assign(category=\"sociodemographic\"),\n",
    "    environmental[[\"medsat_name\", \"type\"]].assign(category=\"environmental\"),\n",
    "    outcomes[[\"medsat_name\", \"type\"]].assign(category=\"outcomes\"),\n",
    "    auxiliary[[\"medsat_name\"]].assign(category=\"auxiliary\"),\n",
    "]\n",
    "\n",
    "variables = pd.concat(variables, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns lack metadata entries but are not of interest for this piece:\n",
    "\n",
    "- `o_OME_per_capita`\n",
    "- `e_skin_reservoir_content`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(medsat.columns) ^ set(variables[\"medsat_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsat.to_parquet(MEDSAT_PATH / \"medsat.parquet.gzip\", index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(\"../data/_tmp/MedSat Variables.xlsx\", MEDSAT_PATH / \"medsat_variables.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sociodemographic.to_csv(MEDSAT_PATH / \"sociodemographic.csv\", **CSV_KWARGS)\n",
    "environmental.to_csv(MEDSAT_PATH / \"environmental.csv\", **CSV_KWARGS)\n",
    "outcomes.to_csv(MEDSAT_PATH / \"outcomes.csv\", **CSV_KWARGS)\n",
    "auxiliary.to_csv(MEDSAT_PATH / \"auxiliary.csv\", **CSV_KWARGS)\n",
    "variables.to_csv(MEDSAT_PATH / \"variables.csv\", **CSV_KWARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an account at https://data.cdrc.ac.uk\n",
    "username = \"cdrowley\"\n",
    "password = getpass(\"CDRC Password: \")\n",
    "\n",
    "cdrc_session = cdrc_login(username, password)\n",
    "if cdrc_session is None:\n",
    "    raise Exception(\"CDRC login failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_list = [\n",
    "    \"https://data.cdrc.ac.uk/system/files/key.csv\",\n",
    "    \"https://data.cdrc.ac.uk/system/files/pen_portraits.json\",\n",
    "    \"https://data.cdrc.ac.uk/system/files/per_geometry.csv\",\n",
    "    \"https://data.cdrc.ac.uk/system/files/spatial_signatures_GB_clean.gpkg_.zip\",\n",
    "]\n",
    "\n",
    "for url in download_list:\n",
    "    out_path = TMP_PATH / url.split(\"/\")[-1]\n",
    "\n",
    "    if out_path.exists():\n",
    "        print(f\"File already exists: {out_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Downloading {url} to {out_path}\")\n",
    "    with cdrc_session.get(url, stream=True) as r:\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Signatures Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_zip = \"../data/_tmp/spatial_signatures_GB_clean.gpkg_.zip\"\n",
    "shutil.unpack_archive(ss_zip, extract_dir=TMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_per_geometry = pd.read_csv(\"../data/_tmp/per_geometry.csv\")\n",
    "ss_pen_portraits = (\n",
    "    pd.read_json(\"../data/_tmp/pen_portraits.json\", orient=\"index\")\n",
    "    .rename(columns={0: \"description\"})\n",
    "    .reset_index(names=[\"type\"])\n",
    ")\n",
    "\n",
    "ss = (\n",
    "    gpd.read_file(\"../data/_tmp/spatial_signatures_GB_clean.gpkg\")\n",
    "    .merge(ss_pen_portraits, on=\"type\", how=\"left\")\n",
    "    .merge(ss_per_geometry, on=\"id\", how=\"left\")\n",
    ")\n",
    "ss.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Signatures Variables / Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = pd.read_csv(\"../data/_tmp/key.csv\").rename(columns={\"Unnamed: 0\": \"name\", \"0\": \"description\"})\n",
    "variables.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = cdrc_get_metadata(\"https://data.cdrc.ac.uk/dataset/spatial-signatures-great-britain\", cdrc_session, 1)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get around 100mb limit on GitHub files\n",
    "chunk_size = 6_000\n",
    "n_chunks = len(ss) // chunk_size + 1\n",
    "SS_PART_DIR = SPATIAL_SIGNATURES_PATH / \"partitioned\"\n",
    "SS_PART_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    chunk = ss.iloc[i * chunk_size : (i + 1) * chunk_size]\n",
    "    chunk_num = i + 1\n",
    "    file_path = SS_PART_DIR / f\"spatial_signatures_{chunk_num:02d}.parquet.gzip\"\n",
    "\n",
    "    chunk.to_parquet(file_path, index=False, compression=\"gzip\")\n",
    "    print(f\"Written chunk {chunk_num:02d} of {n_chunks}, {len(chunk)} rows, {file_path.stat().st_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv(SPATIAL_SIGNATURES_PATH / \"metadata.csv\", **CSV_KWARGS)\n",
    "variables.to_csv(SPATIAL_SIGNATURES_PATH / \"variables.csv\", **CSV_KWARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSOA Lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The region to LSOA lookup table sourced from MedSat does not contain the regions for all LSOAs, the most recent version (updated 03 May 2024) from the Office for National Statistics (ONS) is used as the preferred source. This file also includes lookups for Built-up Areas (BUAs) and local authority districts (LADs).\n",
    "\n",
    "[Link to Source](https://www.data.gov.uk/dataset/c43641d8-710c-48e6-9139-1302953cf16c/lsoa-2021-to-bua-to-lad-to-region-december-2022-best-fit-lookup-in-ew-v2), note this is different to the direct link to the file downloaded below (which is more liable to change over time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONS_PATH = Path(\"../data/ons\")\n",
    "ONS_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "url = 'https://open-geography-portalx-ons.hub.arcgis.com/api/download/v1/items/0352e811ec2c4fc5917f39aea2d1b8a3/csv?layers=0'\n",
    "\n",
    "cols_to_keep = ['LSOA21CD', 'LSOA21NM', 'BUA22CD', 'BUA22NM', 'LAD22CD', 'LAD22NM', 'RGN22CD', 'RGN22NM', 'ObjectId']\n",
    "lsoa_lookup = pd.read_csv(url)[cols_to_keep]\n",
    "\n",
    "lsoa_lookup.to_csv(ONS_PATH / \"lsoa_to_bua_lad_region_lookup.csv\", **CSV_KWARGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
